# 机器学习笔记（基础篇）

本文跟随《机器学习》（周志华）和《统计学习方法》（李航）重新梳理机器学习相关基础知识。
- [机器学习笔记（基础篇）](#机器学习笔记基础篇)
  - [0.数学基础](#0数学基础)
    - [0.1 对向量求导](#01-对向量求导)
    - [0.2 SVD 方法](#02-svd-方法)
      - [0.2.1 广义逆矩阵](#021-广义逆矩阵)
    - [0.3 拉格朗日乘数法](#03-拉格朗日乘数法)
  - [1.经典模型](#1经典模型)
    - [1.1 线性回归](#11-线性回归)
      - [1.1.1 多元线性回归](#111-多元线性回归)
      - [1.1.2 广义线性模型](#112-广义线性模型)
      - [1.1.3 对数几率回归（逻辑回归）](#113-对数几率回归逻辑回归)
    - [1.2 LDA算法](#12-lda算法)
    - [1.3 支持向量机](#13-支持向量机)
      - [1.3.1 核方法](#131-核方法)
      - [1.3.2 软间隔](#132-软间隔)
      - [1.3.3 支持向量回归](#133-支持向量回归)
    - [1.4 朴素贝叶斯分类器](#14-朴素贝叶斯分类器)
    - [1.5 k近邻学习](#15-k近邻学习)
    - [1.6 主成分分析](#16-主成分分析)
      - [1.6.1 核方法](#161-核方法)
    - [1.7 决策树](#17-决策树)
    - [1.8 隐马尔科夫模型](#18-隐马尔科夫模型)
    - [1.9 CART回归树](#19-cart回归树)
    - [1.10 提升树](#110-提升树)
      - [1.10.1 梯度提升树](#1101-梯度提升树)
      - [1.10.2 GBDT](#1102-gbdt)
    - [1.11 因子分解机](#111-因子分解机)
  - [2.机器学习相关理论](#2机器学习相关理论)
    - [2.1 梯度下降算法](#21-梯度下降算法)
      - [2.1.1 具有动量的随机梯度下降（SGDM）](#211-具有动量的随机梯度下降sgdm)
      - [2.1.1 AdaGrad](#211-adagrad)
      - [2.1.2 RMSProp](#212-rmsprop)
      - [2.1.2 Adam](#212-adam)
## 0.数学基础

### 0.1 对向量求导

$\frac{\partial x^T\alpha}{\partial x}=\frac{\partial \alpha^Tx}{\partial x}=\alpha$

$\frac{\partial Ax}{\partial x}=A^T$
### 0.2 [SVD 方法](https://zh.wikipedia.org/wiki/%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3)

奇异值分解（Singular Value Decomposition）。

将实矩阵A分解为以下形式:

$A=U\Sigma V^T$

$U$和$V$是酉矩阵，满足$UU^T=I,VV^T=I$。

$\Sigma$是对角线元素为$A$奇异值的对角阵，其奇异值一般按照降序排列来保证SVD分解的唯一性。
#### 0.2.1 广义逆矩阵
一个普通实矩阵的广义逆矩阵定义为:

$M^{-1}=V\Sigma^{-1}U^T$

其中$\Sigma^{-1}$是$\Sigma$主对角线非零元素取倒数的结果。

### 0.3 [拉格朗日乘数法](https://zh.wikipedia.org/wiki/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E6%95%B0)

将约束条件加入到求解函数中，求解出的新的函数的极值包含原问题在约束下的极值。

一般地，对含n个变量和k个约束的情况，有：

${\displaystyle {\mathcal {L}}\left(x_{1},\ldots ,x_{n},\lambda _{1},\ldots ,\lambda _{k}\right)=f\left(x_{1},\ldots ,x_{n}\right)-\sum \limits _{i=1}^{k}{\lambda _{i}g_{i}\left(x_{1},\ldots ,x_{n}\right)},}$

对这新函数求偏导进而求解极值点来解决原问题。
## 1.经典模型
### 1.1 线性回归
线性回归假设模型为$y=f(x)=wx+b$，这里的x为一维变量。

这部分在高中数学课堂就学习过，现在再重新推导一遍。

假设基本数据集为$\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\}$。

使用均方差作为模型与真实分布的距离量度：

$\sum \limits_{i=1}^N(f(x_i)-y_i)^2=\sum \limits_{i=1}^N(wx_i+b-y_i)^2$

使这个距离最小的模型称为目标模型的方法称为最小二乘法。

显然这是一个二元函数$f(w,b)$。

求一阶偏导得：

$$\begin{align}
&\frac{\partial f(w,b)}{\partial w}=\sum \limits_{i=1}^N2x_i(wx_i+b-y_i)\\
&\frac{\partial f(w,b)}{\partial b}=\sum \limits_{i=1}^N2(wx_i+b-y_i)
\end{align}$$


使偏导为0，联立解得：
$$\begin{align}
&w=\frac{N\sum \limits_{i=1}^Nx_iy_i-\sum \limits_{i=1}^Nx_i\sum \limits_{i=1}^Ny_i}{N\sum \limits_{i=1}^Nx_i^2-(\sum \limits_{i=1}^Nx_i)^2}\\
&b=\frac{1}{N}\sum \limits_{i=1}^N(y_i-wx_i)
\end{align}$$


#### 1.1.1 多元线性回归

与一维线性回归相似有$y=w^Tx+b$，这里的$w,x$都是多元向量。

有数据集$\{([x_1^1,x_2^1,...,x_n^1]^T,y_1),...,([x_1^m,x_2^m,...,x_n^m]^T,y_m)\}$

记矩阵$X=\{X|x_i=[x_1^i,x_2^i,...,x_n^i,1]\},W=[w_1,w_2,...,w_n,b]^T,Y=[y_1,y_2,...,y_m]$

则回归模型记为$Y=XW$

使用均方差，模型与数据之间的距离可以写成：

$$d=||XW-Y||_2^2=(XW-Y)^T(XW-Y)=\sum \limits_{i=1}^m (X_iW-y_i)^2$$

对W[求导](#01-对向量求导)可得：


$$\frac{\partial d}{\partial W}=2X^T(XW-Y)$$

令上式等于0向量，得$W=(X^TX)^{-1}X^TY$。

#### 1.1.2 广义线性模型

$f(x)=g(w^Tx+b)$，$g(\cdot)$为可微单调函数。

其实质是先对$y$做$g^{-1}$变换，然后再进行线性回归。

#### 1.1.3 对数几率回归（逻辑回归）

对数几率回归使用Sigmoid函数：$\frac{1}{1+e^{-x}}$

$$y=\frac{1}{1+e^{-(w^Tx+b)}}$$

也可以写成：

$$ln\frac{y}{1-y}=w^Tx+b$$

对数几率回归执行的是分类任务，所以其标签y不是连续值而是离散值0和1。

所以没办法使用之前的方法求解。

将模型中的连续y值看作是x对应的标签是1的概率。

$$\begin{align}
&p(y=1|x)=\frac{1}{1+e^{-(w^Tx+b)}}=\frac{e^{w^Tx+b}}{1+e^{w^Tx+b}}\\
&p(y=0|x)=1-p(y=1|x)=\frac{1}{1+e^{w^Tx+b}}
\end{align}$$

考虑进行极大似然估计：

$$L(w,b)=\prod \limits_{i=1}^mp(y=y_i|x_i)=\prod \limits_{i=1}^m[p(y=1|x_i)^{y_i}p(y=0|x_i)^{(1-y_i)}]$$

取对数:
$$\begin{align}
lnL(w,b)&=\sum \limits_{i=1}^m[y_i(w^Tx+b)-y_iln(1+e^{w^Tx+b})-(1-y_i)ln(1+e^{w^Tx+b})]\\&=\sum \limits_{i=1}^m[y_i(w^Tx+b)-ln(1+e^{w^Tx+b})]
\end{align}$$

其最大化目标对应于$-lnL(w,b)$的最小化，可以使用梯度下降算法进行优化。

### 1.2 LDA算法

LDA算法也是用于二分类的一个算法。

考虑数据集$\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\}$，y取0或1，表示a类和b类。

LDA的思想是寻找一个方向$w$，将数据集投影到这个方向上，再这个方向上的投影类内聚集，类间分离。

也就是类内方差小，类间距离大。

类间距离使用类平均值距离来衡量:$||w^T(\mu_0-\mu_1)||_2^2$

类内距离:$\sum \limits_{x_i \in X_0}(w^T(x_i-\mu_0))^2+\sum \limits_{x_i \in X_1}(w^T(x_i-\mu_1))^2$

考虑最大化目标：
$$\begin{align}
J&=\frac{||w^T(\mu_0-\mu_1)||_2^2}{\sum \limits_{x_i \in X_0}(w^T(x_i-\mu_0))^2+\sum \limits_{x_i \in X_1}(w^T(x_i-\mu_1))^2}\\ 
&=\frac{w^T(\mu_0-\mu_1)(\mu_0-\mu_1)^Tw}{w^T[\sum \limits_{x_i \in X_0}(x_i-\mu_0)(x_i-\mu_0)^T+\sum \limits_{x_i \in X_1}(x_i-\mu_1)(x_i-\mu_1)^T]w}\\
&=\frac{w^TS_bw}{w^TS_ww}
\end{align}$$


由于是投影，也就是说$w$的大小对优化结果没有影响，不妨设$w^TS_ww=1$。

由拉格朗日乘子法：

$$F(w,\lambda)=w^TS_bw+\lambda (w^TS_ww-1)$$

[求导](#01-对向量求导)得：
$$\begin{align}
&\frac{\partial F}{\partial w}=S_bw+\lambda S_ww=0\\
&\frac{\partial F}{\partial \lambda}=w^TS_ww-1=0
\end{align}$$



其中$S_bw=(\mu_0-\mu_1)(\mu_0-\mu_1)^Tw=\lambda_1 (\mu_0-\mu_1)$

则:$S_ww=\lambda_2 (\mu_0-\mu_1) \Rightarrow w=S_w^{-1}(\mu_0-\mu_1)$

上式省去系数的原因是$w$的大小对结果没有影响。

由于$S_w$不一定有逆，可以考虑[SVD方法](#02-svd-方法)求解其[广义逆矩阵](#021-广义逆矩阵)。

### 1.3 支持向量机

支持向量机的基本思想是求一个超平面$w^Tx+b=0$来将数据按照其标签分割开，属于二分类学习算法。

假设基本数据集为$\{(x_1,y_1),(x_2,y_2),...,(x_m,y_m)\}$，$y$取为1和-1，并且数据集线性可分。

显然，这样的超平面有无数个，我们要找到其中最好的一个。

什么是最好呢？

自然是能够分开还能够同时远离这两类数据，或者说是在两类数据最中间。

考虑更强的分割条件：
$$\begin{align}
&w^Tx_i+b\ge +1 ,y_i=+1\\
&w^Tx_i+b\le -1 ,y_i=-1
\end{align}$$


如果让这两个超平面之间的间隔最大化，这样就能容忍更多不在训练数据里并且靠近另外一类数据的样本，这个时候距离分割平面最近的样本会出现在这两个超平面上，这些样本点被称为支持向量。

这两个平面的距离为:$\frac{2}{||w||}$

推导很简单，取平面$w^Tx_i+b-1=0$上一点x，然后由我们幼儿园学过的点到超平面的距离得到这个点与另外一个超平面的距离：

$$d=\frac{|w^Tx+b+1|}{||w||}=\frac{|0+1+1|}{||w||}$$

$\max \frac{2}{||w||}$就是我们的优化目标，其等价于$\min \frac{1}{2}||w||^2$。

并且有条件:$y_i(w^Tx_i+b)\ge 1$

对于这种由条件约束的函数最小值计算，很自然想到了小学学过的拉格朗日乘数法。

$$L(w,b,\alpha)=\frac{1}{2}||w||^2+\sum \limits_{i=1}^m\alpha_i(1-y_i(w^Tx_i+b))$$

其中$\alpha$为拉格朗日乘数组成的向量，$\alpha_i\ge0$。

求偏导并且置零得到：
$$\begin{align}
&w+\sum \limits_{i=1}^m-\alpha_iy_ix_i=0\\
&-\sum \limits_{i=1}^m\alpha_iy_i=0
\end{align}$$


将这两个式子带入$L(w,b,\alpha)$:
$$\begin{align}
L(w,b,\alpha)&=\frac{1}{2}(\sum \limits_{i=1}^m\alpha_iy_ix_i)^2+\sum \limits_{i=1}^m\alpha_i(1-y_i((\sum \limits_{i=1}^m\alpha_iy_ix_i)^Tx_i+b))\\
&=\frac{1}{2}\sum \limits_{i=1}^m\sum \limits_{j=1}^m\alpha_iy_i\alpha_jy_jx_i^Tx_j+\sum \limits_{i=1}^m\alpha_i-\sum \limits_{i=1}^m\alpha_iy_i((\sum \limits_{i=1}^m\alpha_iy_ix_i)^Tx_i+b)\\
&=\frac{1}{2}\sum \limits_{i=1}^m\sum \limits_{j=1}^m\alpha_iy_i\alpha_jy_jx_i^Tx_j+\sum \limits_{i=1}^m\alpha_i-\sum \limits_{i=1}^m\sum \limits_{j=1}^m\alpha_iy_i\alpha_jy_jx_i^Tx_j-b\sum \limits_{i=1}^m\alpha_iy_i\\
&=\sum \limits_{i=1}^m\alpha_i-\frac{1}{2}\sum \limits_{i=1}^m\sum \limits_{j=1}^m\alpha_iy_i\alpha_jy_jx_i^Tx_j\end{align}$$


$\max\limits_\alpha\sum \limits_{i=1}^m\alpha_i-\frac{1}{2}\sum \limits_{i=1}^m\sum \limits_{j=1}^m\alpha_iy_i\alpha_jy_jx_i^Tx_j (s.t. \sum \limits_{i=1}^m\alpha_iy_i=0,\alpha_i\ge0)$为原问题的对偶问题。

很好理解，考虑到$\sum \limits_{i=1}^m\alpha_i(1-y_i((\sum \limits_{i=1}^m\alpha_iy_ix_i)^Tx_i+b))\le 0$，有

$$L(w,b,\alpha)\le L(w,b)$$

$L(w,b)$最小化对应于$L(w,b,\alpha)$最大化，也就是原问题的对偶。

模型会变成$y=\sum \limits_{i=1}^m\alpha_iy_ix_i^Tx+b$

通过之前的讨论，可以知道最终只会保留2个(理想情况)支持向量，所以$\alpha$中最后只有两个元素不为0并且值相等。

更加一般的情况应该是有3个支持向量。

由于对对偶问题的求解时间复杂度正比于训练样本数，所以需要一些有技巧的算法来进行求解。

SMO算法，每次只选择一对$\alpha_i,\alpha_j$进行优化。

因为有$\sum \limits_{i=1}^m\alpha_iy_i=0$条件约束，所以优化时实际上是二次函数的优化，这样的优化是非常简单的。

那怎么选取$\alpha_i,\alpha_j$？

首先选择一个违背KKT条件最离谱的。

KKT：

$$\begin{align}
&\alpha_i\ge 0\\
&y_if(x_i)\ge 1\\
&\alpha_i(y_if(x_i)-1)=0
\end{align}$$



然后选择一个使目标函数增长最快的，但是因为这样选择的代价太大，所以使用启发式的选取规则：选择对应样本点离之前选择的$\alpha_i$对应样本点远的。

而对应的$b$的求解可以通过带入一个支持向量的值到$y_s(\sum \limits_{i=1}^m\alpha_iy_ix_i^Tx+b)=1$就可以求解出b。

而为了更好的鲁棒性，会选择对所有支持向量求解出的b取平均值。

#### 1.3.1 核方法

简单来说就是对$x$先用某个函数转换，然后将转换后的结果运行支持向量机。

但是由于最后都会化成与支持向量与其他向量的内积，所以可以转化为使用函数$k(x_i,x_j)$，这里的核函数的结果是一维实数。

对于能够作为核函数的条件为对于任意数据$D={x_1,x_2,...,x_m}$矩阵$K_{i,j}=k(x_i,x_j)$是半正定矩阵，并且核函数是对称函数。

常用的核函数有：

|名称|表达式|
|---|---|
|线性核|$x_i^Tx_j$|
|多项式核|$(x_i^Tx_j)^d$|
|高斯核|$\exp(-\frac{\|x_i-x_j\|^2}{2\sigma^2})$|
|拉普拉斯核|$\exp(-\frac{\|x_i-x_j\|}{\sigma})$|
|Sigmoid核|$tanh(\beta x_i^Tx_j+\theta)$|

#### 1.3.2 软间隔

之前的支持向量机的推理有一个非常强的前提条件：数据集线性可分。

但是在实际应用中并没有这么多线性可分的数据集，这就要求支持向量机能够容忍一些错误。

在原有损失函数上加入正则项：

$$\min\limits_{w,b}\frac{1}{2}||w||^2+C\sum\limits_{i=1}^ml_{0/1}(y_i(w^Tx_i+b)-1)$$

当$z<0,l_{0/1}(z)=1$，否则$l_{0/1}(z)=0$

但是这样的函数不可微，所以通常使用近似可微函数代替。
|名称|形式|
|---|---|
|hinge函数|$\max(0,1-z)$|
|指数损失|$e^{-z}$|
|对率损失|$\log(1+e^{-z})$|

为什么加入正则项可以让其容忍一些不对的数据点呢？如果强行当成线性可分的数据集，就会在训练之中让间隔变得很小，从而第一项会变大，如果过于容忍异常点，虽然间隔会变大，但是其第二项损失就会变大很大。

需要在两个目标中进行合适去取舍来达到整体的最优。

引入松弛变量$\xi_i$：

$$\min\limits_{w,b}\frac{1}{2}||w||^2+C\sum\limits_{i=1}^m\xi_i$$

其约束条件更改成：

$$y_i(w^Tx_i+b)\ge1-\xi_i,\\\xi_i\ge0$$

真正让其容忍的原因在于约束条件变弱。

#### 1.3.3 支持向量回归

### 1.4 朴素贝叶斯分类器

朴素贝叶斯分类器假设数据x的所有属性相互独立，也就是说每一个属性对于分类结果都是有独立的贡献，就比如说分类人的胖瘦应该是将身高和体重放在一起考虑，但是在这个假设下就是分开考虑身高和体重的影响。

朴素贝叶斯属于多分类学习算法，假设数据集空间为$x\in \mathbb{R}^{m\cdot n},c\in \mathbb{R}^k$。c表示类别（class）。

考虑使学习目标为$\max P(c|x)$

在独立性假设下可得：$P(c|x)=\frac{P(c)P(x|c)}{P(x)}=\frac{P(c)}{P(x)}\prod \limits_{i=1}^nP(x_i|c)$

对于每一个类别$P(x)$都是相同的，因为学习的时候数据集就一个。

所以学习目标可以写成:$\max P(c)\prod \limits_{i=1}^nP(x_i|c)$

其中$P(c)$由于我们没有数据集的概率分布，所以可以使用数据集的统计分布来估计。

$$P(c_j)=\frac{\sum \limits_{i=1}^m \mathbb{I}(c=c_j|x^{(i)})}{m}$$

对于离散取值的$x_i$:

$$p(x_t=p_{tl}|c_j)=\frac{\sum \limits_{i=1}^m \mathbb{I}(c=c_j,x_t=p_{tl})}{\sum \limits_{i=1}^m \mathbb{I}(c=c_j|x^{(i)})}$$

其中$p_{kl}$是$x$第$t$个属性的第$l$个离散取值。

对于连续的$x_i$，可以假定其分布服从高斯分布$p(x_t|c_j)\sim N(\mu_{c_j},\sigma_{c_j}^2)$

$$p(x_t|c_j)=\Phi_{\mu_{c_j},\sigma_{c_j}}(x_t)$$

由于连乘概率会可能会导致下溢，通常对目标函数取对数。

同时由于出现这种可能，理论上$x_k$可以取值为$p_{kl}$但是在数据集里没有这样的样本就会导致其统计分布为0，作为概率带入的话会造成灾难（连乘结果为0），所以可以使用拉普拉斯修正。

$$\hat{P}(x_t=p_{tl}|c_j)=\frac{\sum \limits_{i=1}^m \mathbb{I}(c=c_j,x_t=p_{tl})+1}{\sum \limits_{i=1}^m \mathbb{I}(c=c_j|x^{(i)})+N_k}$$

$$\hat{P}(c=c_j)=\frac{\sum \limits_{i=1}^m \mathbb{I}(c=c_j|x^{(i)})+1}{m+k}$$

从上面的这些概率中可以得到k个$P(c=c_j|x^{(i)})$，选取其中最大的概率，其对应的类别就是朴素贝叶斯分类器的分类结果。


半朴素贝叶斯分类器放松了独立性的假设，改成一个属性最多依赖于另外一个属性。
### 1.5 k近邻学习
k近邻算法的思想非常简单，获取一个数据后寻找离这个数据最近的k个数据，通过平均法或者投票法得出关于这个输入数据的结果。

这个算法没有显示的学习过程，仅仅是将训练数据保存起来，也就是说它没有训练过程。

而其参数也只有k以及距离量度的选取，这两个因素决定了k近邻模型的性能，同时距离量度的选取是受到理论知识的指导的。

### 1.6 主成分分析
主成分分析是经典的降维算法。

以下推理基于个人理解，与教科书上略有差别。

我们希望在数据空间中找到这么一个超平面$0=w^Tx+b$，使得所有样本点在上面有很好的分布，同时这些样本点距离超平面的距离也非常近。

基于这样一个平面，我们可以认为其在超平面上的投影坐标是主要属性，而其与这超平面的距离为次要属性，因为样本点距离这个超平面很近。

那么原本的数据就可以写成$(x_1,x_2,...m,x_n)->(y_1,y_2,...,y_{n-1},d)$，其中d有正负之分，d为样本点$x$在$w$上的投影，超平面的维数实际上要比样本点维度数底一维。

可以想象到，这实际上是一个坐标变换$y=Ax$。

不断重复这个过程，我们就能得到n个相互垂直的$w_i$，使得x在这些向量方向上的投影按照规模从大到小依次排列。

这样的话，在投影规模比较小的方向上其值对于整体样本点的描述没有多大贡献，忽略掉对数据使用影响不大，而在实际使用中只需要规模比较大的一些方向上的投影就能较好的描述这个数据了。

这样的一个正交坐标变换将原本的数据空间转变成一个坐标值影响力依次递减的坐标。

举一个例子，假设有一个能二维数据集，其在坐标系中大致呈现直线，也就是说如果使用线性回归来拟合会有很好的效果，现在我们把样本点投影在一条直线上，得到一个坐标$t$，然后记其与直线的距离为$d$，那么其原坐标就是直线上$t$对应的点坐标加上$d$倍的直线法向量。

而我们的目标就是找到一条直线，当我把d变成0后，使用(t,0)还原的坐标离原数据点坐标最近。

显然，线性回归的直线会是一条性能很好的目标直线。

假定数据集结果了中心化：$\hat{x}_i=x_i-\bar{x}$

正交变换$y=Wx$可以实现我们上述的目标，那么会有$W^Ty'\approx x,y'=[y_1,y_2,...,y_d,0,0,...,0]^T$

也就是$\min \limits_W ||W^TY'-X||$,$X,Y$是m个数据点组成的矩阵。

假设使用平方距离来衡量，则有$\min \limits_W||W^TY'-X||_2^2$。

注意到，当样本数据足够多的时候，有$r(X)-n,r(Y')=d$。

这显然是一个最优k秩近似问题。

其解为$W^TY'=U\Sigma_dV^T$，令$W=U^T,Y'=\Sigma_dV^T$就是我们需要的结果。

也就是说先将$X$进行分解，然后保留最大的$d$个奇异值，然后取$\Sigma_dV^T$作为降维后的结果。

这么理解主成分分析这件事情呢？

主要在于通过这种方法你可以换个角度看待数据。

比如说我在玩某一个网络游戏，游戏中的角色有[攻击力,防御力,生命中]三个属性，对于原本数据你可能看到的事情是我攻击力高但是防御力和生命中不高。

可以考虑到一些影响这些属性的因素比如说人物等级，每一级等级可以提供的一些属性$等级*[5,3,50]$，武器可以提供属性：$武器等级*[20,0,0]$，防具：$防具等级*[0,10,120]$。

如果能够把总属性分解成[等级，装备等级，防具等级]，就能看到不同的东西，从这个角度看待数据的话会对接下来应该干什么作出指导。

通过这个例子可以看出其中主成分分析的意义，当然主成分分析后的数据不像上面的例子那样有明确的意义，更多的是数学上分解。

#### 1.6.1 核方法

考虑将原数据空间的$x\in \mathbb{R}^n$映射到其他维度空间$y\in \mathbb{R}^l,y=\Phi(x)$。

然后将映射之后的数据进行PCA分解，对于其中步骤当中有：

$$YY^Tw_j=\lambda w_j=\sum \limits_{i=1}^m(y_iy_i^T)w_j$$

$$Y=[y_1,y_2,...,y_m]$$

令$A_{i,j}=\frac{y_i^Tw_j}{\lambda}$

则上述等式可以写成：

$$w_j=\sum \limits_{i=1}^my_iA_{i,j}=YA_{(\cdot),j},w=YA$$

引入核函数$k(x,y)=\Phi(x)^T\Phi(y)$

记核函数矩阵$K_{i,j}=k(x_i,x_j)$

$$\lambda A_{i,j}=y_i^T\sum \limits_{k=1}^my_kA_{k,j}=\sum \limits_{k=1}^mK_{i,k}A_{k,j}=K_{i,(\cdot)}^TA_{(\cdot),j}$$

$$\lambda A_{(\cdot),j}=KA_{(\cdot),j}$$

即A为K的特征向量矩阵。

考虑将$y_i$投影到低维空间，$z^{(i)}_j=w_j^T\Phi(x_i)=\sum \limits_{k=1}^my_k^Ty_iA_{k,j}=\sum \limits_{k=1}^mK_{k,j}A_{k,j}$

也就是说需要计算的有两个矩阵：核矩阵K以及K对应的特征值矩阵，并且应该进行正交标准化。

可以看到，PCA的核方法的时间复杂度和空间复杂度要求都比较高，其中核矩阵有$m^2$个数据，对于简单的一万个数据就需要一亿个浮点数存储空间，而A具有相同的大小。而在时间复杂度上，在计算K以及求解A的过程中就会需要大量的计算资源，并且最后求降维后数据时还得有$O(dm^2)$的浮点乘法与加法运算。

### 1.7 决策树

决策树是经典的算法，主要用于分类问题。

决策树的形式很像我们写程序时写出的if-else树，对于某一个条件进行判断，然后进入判断子树，最终在叶子结点上得到结果。

比如说if w>50:class='胖'else:class='廋'就是一个简单的决策树。

当然，决策树的结构不是想怎么来就怎么来，是有理论依据的。

信息熵定义为$Ent(D)=-\sum \limits_{k=1}^{|y|}p_k\log_2p_k$

$p_k$为第k类样本的比例。

通过简单的数学分析知道，当样本越纯净，也就是$p_k$靠近0或1，信息熵就越接近0。

如果决策树由判断条件$a=a^v$产生出了新的分支，那么我们会希望在分支中的样本更加纯净，也就是说原本分支里有三类的样本，但是在这个分支上假设新的分支后，变成两类了，或者是其中某一类的含量接近于0。

选择可以参与划分分支的属性中使$Gain(D,a)=Ent(D)-\sum\limits_{v=1}^V\frac{|D^v|}{|D|}Ent(D^v)$最大的属性$a$,$|D|$为D中样本数量，不难看出其目的是使得增加分支后信息熵减小的最多。

但信息熵是有缺点的，因为取值多的属性会更加受到青睐。

于是，一个改进的信息增益被称为增益率的目标出现了：$Gain_ratio(D,a)=\frac{Gain(D,a)}{-\sum\limits_{v=1}^V\frac{|D^v|}{|D|}\log_2\frac{|D^v|}{|D|}}$

定义Gini指数:$Gini(D)=1-\sum\limits_{k=1}^{|y|}p_k^2$

用这个指数代替信息熵就得到了基于基尼指数的信息增益公式。

CART分类树就是使用基尼指数的。

剪枝问题：

决策树按照属性进行划分后会出现过拟合问题，简单来说就是分的太细了，不具有泛化能力，比如说150-160斤和160-170斤都是肥胖，把它们分开是没有意义的，所以需要把多余的分支去掉。

剪枝策略有预剪枝和后剪枝。

预剪枝简单来说就是添加分支的过程中考虑准确率，如果添加分支会使准确率下降就不添加，这种方法会尽量不展开分支导致，这是一种贪心的策略，可能导致欠拟合问题。

后剪枝的方法是将某一个子决策树变成分支，如果准确率上升了，那就保留更改，如果没有上升就保持原来的结构。

一般来说，后剪枝无论是时间复杂度还是性能上都优于预剪枝。


### 1.8 隐马尔科夫模型
![](images/马尔可夫模型示意图.png)（图片来自维基百科）

如图，$x_t$为一连串的状态，而$y_t$为每一个状态的输出，这样的模型叫做隐马尔科夫模型。

马尔可夫模型假设的是当前状态只和上一个状态有关，与之前的状态无关。

那么一个具体流程发生的概率就会表示为：

$$P(x_1,y_1,x_2,...,x_n,y_n)=P(x_1)P(y_1|x_1)\prod \limits_{i=2}^n P(x_i|x_{i-1})P(y_i|x_i)$$

很好理解，$P(x_1)$表示初始状态产生的概率，$P(y_i|x_i)$是当前状态得到观测值的概率，$P(x_i|x_{i-1})$是从上一个状态转移到当前状态的概率。

就那语音转文字来举例：一段音频，可能第一个音是wo，但不是每一个音频都是这个发音为开头的所以有初始音节的概率。

然后根据听到的第一个音节，你可能得到文字：我(0.9)、喔(0.01)、沃(0.01)……

再接下来wo后面可能接的是shi，也可能是cao，也可能是zai……，总之也是一个条件概率。

假设状态$x_i$的取值为$\{s_1,s_2,...,s_m\}$

那么，状态转移的几率就可以表达为一个矩阵$A:a_{ij}=P(x_t=s_i|x_{t-1}=s_j)$

以及输出观测矩阵$B:b_{ij}=P(y_t=o_i|x_t=s_j)$

还有初始状态概率向量$\pi:\pi_i=P(x_1=s_i)$

### 1.9 CART回归树

CART回归树是一种决策树，但与决策树不同的是，CART回归树是输出的值是连续的，类似于逻辑回归，用连续值逼近离散值。

CART是一颗二叉树，每一个节点都会将产生一个切分点，把输入数据分成两部分，然后交给子树继续决策。

对于某一个节点来说，最好的切分点是能让两颗子树的拟合损失最小的值。

这里我们用均方差来表示损失。

既然说CART回归树的节点是连续的，那这个值是怎么产生的呢？

在训练过程中，叶节点会得到一些样本，为了让叶节点上的均方差损失最小（训练损失），我们取这些样本的目标值（target）的平均值作为叶节点的输出。

为什么要让叶节点上的损失最小呢？从而采取平均值作为输出呢？

那就是排除输出策略的影响。你们找损失大的原因？关我叶节点什么事？我已经do my best啦啊，你得把锅甩给上面决策的节点。

在叶节点上面的节点在找到最佳的切分点的时候又能把锅甩给再上面的节点，于是锅就这样一步一步的被甩给最上面的节点，这是一种递归的过程。

但是要考虑的是，当上层节点的策略更新以后，其子节点的策略就不一定是最优的了，所以我们需要设计算法让整个决策树到最优或者近似最优。

最小二乘回归树生成算法是一个基于贪心策略的生成算法，从根节点向下生成，先组成一个根节点与两个叶节点的树，然后找到最优的切分变量和切分点。接下来分别对两个叶节点使用同样的策略生成子树。直到树满足一定的条件，比如说准确率达到多少多少。

另外一点就是，特征值是连续的，而且可以重复使用，所以CART回归树很可能需要进行剪枝。

### 1.10 提升树

提升树是一种基于决策树的提升算法。

假设现在我们在训练集上得到了一个回归树，这颗回归树的规模与性能都不错，但是我们希望能够继续提升模型的准确率。

现在我们将训练集上的target与模型的预测值做差，将训练集的target换成差值就能得到一个新的训练集，接下来我们在这个新的训练集上训练一个新的模型。

新的模型学习的是前一个模型的误差，将两个模型的预测值相加就是最终的输出，这就是提升树。

而只要误差存在，这个算法就能继续用，得到更多回归树。

这样多决策树组成的模型的最大优势就是可以并行计算，因为决策树是一步一步决策的，难以进行并行计算。而提升树可以利用多线程并行计算，在基本不增加运算时间的情况下提升准确率。

#### 1.10.1 梯度提升树

梯度提升树在提升树基础上做了一点改进，将误差替换成了损失函数对于预测值的负梯度。

在使用均方差的二分之一作为损失函数的时候，其负梯度就是target与预测值的差。

#### 1.10.2 GBDT

GBDT算法其实就是CART回归树加上梯度提升方法。

### 1.11 因子分解机

什么是因子分解机呢？

复习一下线性回归，线性回归的模型是$y=WX$。

在$x$当中的每一个分量都对$y$有独立的贡献。但是没有考虑分量之间的联合影响。

为了考虑到，分量之间的联合影响，所以在线性回归模型的基础上增加了分量之间的乘积。不过我觉得复杂的关系只用乘积来表示也太片面了，不如把理由改成增加模型复杂度？

这里我们只考虑二阶的特征组合，更高阶的很烧脑的！

所以我们的模型可以写成

$$y=w_0+\sum \limits_{i=1}^n w_ix_i + \sum \limits_{i<j}w_{i,j}x_ix_j$$

但是考虑到稀疏编码的问题，$x_ix_j$在很多时候都是0，导致$w_{i,j}$难以更新。

可以把二阶权值矩阵看作是一个对称矩阵，而我们只用到了其中的一半作为参数，在这种情况下再将权值矩阵用矩阵$VV^T$代替，$V$是一个$n$x$k$的矩阵，当$k$足够大的时候，$VV^T$就能足够接近原权值矩阵。

那么其模型就变成了：

$$y=w_0+\sum \limits_{i=1}^n w_ix_i+\sum \limits_{i=1}^n\sum \limits_{j=1+i}^n<V_i,V_j>x_ix_j$$

这样，在更新的时候，对于一个$x_i\neq 0$只需要再找到另一个$x_k\neq 0$就能更新$V_i,V_j$。

原来的需要更新的目标有$n(n-1)$个，而现在的目标只有$n$个（一次更新整个向量）。

$$\begin{align} \sum_{i=1}^{n}\sum_{j=i+1}^{n}<\textbf{v}_{i},\textbf{v}_{j}>x_{i}x_{j} &=\frac{1}{2}\left(\sum_{i=1}^{n}\sum_{j=1}^{n}<\textbf{v}_{i},\textbf{v}_{j}>x_{i}x_{j}-\sum_{i=1}^{n}<\textbf{v}_{i},\textbf{v}_{i}>x_{i}x_{i}\right)\\ &=\frac{1}{2}\left(\sum_{i=1}^{n}\sum_{j=1}^{n}\sum_{f=1}^{k}v_{i,f}v_{j,f}x_{i}x_{j}-\sum_{i=1}^{n}\sum_{f=1}^{k}v_{i,f}v_{i,f}x_{i}x_{i}\right)\\ &=\frac{1}{2}\sum_{f=1}^{k}\left(\left(\sum_{i=1}^{n}v_{i,f}x_{i}\right)\left(\sum_{j=1}^{n}v_{j,f}x_{j}\right)-\sum_{i=1}^{n}v_{i,f}^{2}x_{i}^{2}\right)\\ &=\frac{1}{2}\sum_{f=1}^{k}\left(\left(\sum_{i=1}^{n}v_{i,f}x_{i}\right)^{2}-\sum_{i=1}^{n}v_{i,f}^{2}x_{i}^{2}\right)\end{align}$$

经过这个转换，之前的运算量是$\frac{n(n-1)(k+2)}{2}$次乘法运算与$\frac{n(n-1)(k-2)}{2}-1$次加法运算。变形之后的运算量是$3nk+1$次乘法和$2nk-1$次加法运算。可以看出，只要$n>7$这就是一件稳赚不赔的生意。

所以表达式重写为：

$$y(x)=w_{0}+\sum_{i=1}^{n}w_{i}x_{i}+\frac{1}{2}\sum_{f=1}^{k}\left(\left(\sum_{i=1}^{n}v_{i,f}x_{i}\right)^{2}-\sum_{i=1}^{n}v_{i,f}^{2}x_{i}^{2}\right)$$

对各个学习参数求导可得：

$$\frac{\partial \hat{y}(x)}{\partial \theta}=\begin{cases}
1 ,& \text{if $\theta$ is $w_{0}$} \\
x_{i}, &\text{if $\theta$ is $w_{i}$} \\
x_{i}\sum_{j=1}^{n}v_{j,f}x_{j}-v_{i,f}x_{i}^{2} .& \text{if $\theta$ is $v_{i,f}$}   
\end{cases}$$

但是，我又要但是了，这个方法是为了避免稀疏数据没办法学习而设计的，是不得已的办法。用$V$代替原有矩阵会损失一部分函数空间，所以遇到不稀疏的数据还是使用原始的形式比较好。


## 2.机器学习相关理论
### 2.1 梯度下降算法

梯度下降算法是机器学习当中最重要的算法，没有之一。

梯度下降算法是非常直觉的。

考虑函数连续可导函数$f(x)$，根据我们幼儿园学过的导数知识知当$f'(x)>0,f(x)$单调递增，反之亦然。

如果导数大于零，想让函数值减小就需要减小$x$的值，反之亦然。

所以有$\exists \epsilon > 0$使$f(x-\epsilon f'(x)) \le f(x)$

使用一个较小的学习率$\alpha$代替$\epsilon$，然后将其推广到多元函数就得到了梯度下降算法。

梯度实质上就是多元变量向量对于的一阶偏导数向量，沿着梯度的方向函数值增加地最快。

当然，使用学习率代替$\epsilon$后不能保证函数值一定会减小，但是可以用一个概率$p$来表示函数值减小的概率，学习率越小、函数越平滑，这个概率就会越接近1。

从另一个方面解释梯度下降算法：

考虑将函数$f(x)$写成泰勒公式形式：

$$f(x)=f(x_0)+(x-x_0)f'(x_0)+\frac{1}{2}(x-x_0)^2f''((1-\theta)x_0+\theta x)$$

其中$\theta \in (0,1)$，这个公式在$x_0$的领域$x\in (x_0-\epsilon,x_0+\epsilon)$内成立。

在这个邻域内，因为$x-x_0$非常接近0，只要二阶导数不是无穷大，后面一项就可以忽略掉。

$$f(x)\approx f(x_0)+(x-x_0)f'(x_0)$$

推广到二阶就有:

$$f(x,y)\approx f(x_0,y_0)+\frac{\partial f(x,y)}{\partial x}(x_0)(x-x_0)+\frac{\partial f(x,y)}{\partial y}(y_0)(y-y_0)$$

邻域定义为$(x-x_0)^2+(y-y_0)^2\le \epsilon^2$

显然，离$(x_0,y_0)$越远，函数值改变的就越多，所以无论是最大值还是最小值都会在邻域边缘上取得。

则$\frac{\partial f(x,y)}{\partial x}(x_0)(x-x_0)+\frac{\partial f(x,y)}{\partial y}(y_0)(y-y_0)=\epsilon(\frac{\partial f(x,y)}{\partial x}(x_0) cos \theta +\frac{\partial f(x,y)}{\partial y}(y_0) sin \theta)$

由幼儿园学过的辅助角公式得：$\epsilon \sqrt{(\frac{\partial f(x,y)}{\partial x}(x_0))^2+(\frac{\partial f(x,y)}{\partial y}(y_0))^2}sin(\theta + \varphi)$

当其取最小值的时候，$f(x,y)$也是最小值，所以

$$\frac{y-y_0}{x-x_0}=\frac{\epsilon \sin \theta}{\epsilon \cos \theta}=\tan \theta$$

当$\theta = -\frac{\pi}{2}-\varphi$时取最小值。

$$\frac{y-y_0}{x-x_0}=\tan(-\frac{\pi}{2}-\varphi)=\tan(\frac{\pi}{2}-\varphi)=\cot \varphi=\frac{\frac{\partial f(x,y)}{\partial y}(y_0)}{\frac{\partial f(x,y)}{\partial x}(x_0)}$$

如此，可以说明最小值所在的方向与梯度的方向相同或相反，带进去一解就知道，最小值所在的方向与梯度方向相反。

所以梯度下降可以理解为一个贪心算法，每一步只考虑最小值。

梯度下降存在不少问题，最突出的就是局部最小值问题。

#### 2.1.1 具有动量的随机梯度下降（SGDM）

模仿物理学中的惯性，就得到了SGDM。

在梯度下降算法运行时保留上一次更新参数时的更新方向，作为梯度下降的原有“速度”。

下一次更新的方向为$m_t=\beta m_{t-1}+(1-\beta)g_t$,$g_t$为第t次更新时的梯度，$m_{t-1}$为保存的动量，也是上一次更新的方向。

#### 2.1.1 AdaGrad

SGD还有一个问题，那就是局部震荡。

考虑二次函数$y=100x^2$

```python {cmd matplotlib hide run}
import matplotlib.pyplot as plt
import numpy as np
x = np.arange(0, 10, 0.1)
y = 100* (x-5)**2
plt.figure(figsize=(4, 6))
plt.axis('off')
plt.plot(x,y)
plt.show() # show figure
```
对于$x=5$其导数为1000，假设我现在的学习率刚好是0.01，那么一次更新后，$x=-5$。这种情况，$x$就会在-5和5之间反复横跳。

直觉的解决办法是减小学习率，但是即便减小学习率，这种情况还是可能会出现。

而学习率0.01可能对于这个变量来说比较大，但是可能对其他变量来说非常小。

这就引出了一个SGD的弊病：所有的变量使用相同的学习率来更新。

那就不能为每一个变量分配一个合适的学习率吗？

答：你行你来，先不论函数形状实际上比这复杂不知道多少，就说那工作量……

于是自适应学习率的算法被提出来了。

$$\theta_i^t=\theta^{t-1}_i-\frac{\eta}{\sqrt{G_{i}^{t-1}+\epsilon}} \nabla_{\theta_i^{t-1}}J(\theta)$$

其中$G_{i}^{t-1}$为$\theta_i$的历史梯度平方和，$\epsilon$为一个极小值，防止分母为0。

可以看到，如果上面的二次函数使用这个算法来更新，那就不会出现震荡的问题。

当然，自适应算法并不是说一定能够避免震荡问题，只不过是把一部分震荡问题解决了。

可以看到，当梯度特别大的时候，第二项分母也会变大，这样就不会步子太大。

但是，其也是有缺点的，比如说经历了梯度特别大的地区，然后在平缓地区的时候更新速度会变得特别慢。

#### 2.1.2 RMSProp
上诉自适应算法有经历了梯度特别大的地区然后在平缓地区的时候更新速度会变得特别慢的问题。

导致这个问题的原因是历史梯度平方和只会越来越大，但是很久以前的梯度是多少对于现在的更新来说是没有什么意义的。

RMSProp在分母上应用了动量的更新方式：

$$\begin{align}
&\theta_t=\theta_{t-1}-\frac{\eta}{\sqrt{v_t}}g_{t-1}\\
&v_1=g_0^2\\
&v_t=\alpha v_{t-1}+(1-\alpha)g_{t-1}^2
\end{align}$$

这样的分母也可以处理震荡问题，但是却摆脱了自适应学习率的缺点。

但是，AdaGrad还有另外一个优点，那就是学习率随着更新进程渐渐减小，刚开始学习率大快速为模型定下基调，后面慢慢减小学习率精细调整。

#### 2.1.2 Adam

Adam 可以说是目前最好的梯度下降算法，它综合考虑了动量、震荡、更新进程。

Adam有以下特点：
>1. 使用动量
>2. 学习率随梯度增大而减小
>3. 学习率随更新进程减小

$$\begin{align}
&\theta_t=\theta_{t-1}-\frac{\eta}{\sqrt{\hat{v}_t}+\epsilon}\hat{m}_t\\
&\hat{m}_t=\frac{\hat{m}_t}{1-\beta_1^t}\\
&m_t=\beta m_{t-1}+(1-\beta)g_t\\
&\hat{v}_t=\frac{v_t}{1-\beta_2^t}\\
&\beta_1=0.9\\
&\beta_2=0.999\\
&\epsilon=10^{-8}
\end{align}$$

多的就不说了，更新的时候默认使用Adam就行了。




 

