本文的目标是在不借助任何机器学习数据包的情况下搭建一些简单的神经网络，并且能够进行训练。

不借助任何机器学习包是指在神经网络模型的构建、计算、训练方面不使用pytorch、tensorflow等工具包，仅仅依赖math、random等自带的系统库，甚至连numpy都会尽可能少使用。

而在数据的收集、预处理、展示、保存等方面可能会使用其他工具包，比如说数据预处理和数据集下载等操作。

至于性能方面，代码仅仅用于展示，实用性不大，也不会有任何关于异常处理的代码。

# 1 构建自动微分系统

自动微分系统听起来复杂，但其实需要求导的只有基本初等函数，剩下的都是使用求导法则来处理。

首先创造一个带梯度的数值类:

```python
class MyFloat:#基本元素
    def __init__(self,value=0,require_grad=False,operation_graph=None):
        self.value=value
        self.require_grad=require_grad
        self.grad=0.0 #梯度仅仅是梯度，而不是对谁的梯度，对是谁的梯度由最高层调用者定义
        self.operation_graph=operation_graph
    def backward(self,grad=None):
        if self.require_grad == False or grad == 0:
            return
        if grad is not None:
            self.grad+=grad
            if self.operation_graph is not None:
                self.operation_graph.backward(grad)
        elif self.operation_graph is not None:
            self.operation_graph.backward(1.0)
```
自动微分的微分主要依赖于计算图，下面实现计算图。

```python
import math
class OperationGraph:#计算图
    def __init__(self,op_type=None,obj_list=None,other=None):
        self.op_type=op_type   #运算类型标识符
        self.obj_list=obj_list #参与运算的对象
        self.other=other       #辅助求导的一些量
    def backward(self,grad):
        if self.op_type=='sum':#其实包含了减法，但用加法实现减法的效率低
            for obj in self.obj_list:
                obj.backward(grad)
        elif self.op_type=='multiply':#other 储存了积，避免再次求积
            for obj in self.obj_list:
                obj.backward(grad*self.other[0]/obj.value)
        elif self.op_type=='power':#不仅对指数求导，对底数也求导，可以实现幂指函数、幂函数、指数函数求导，其中除法被包含在幂函数运算里
            self.obj_list[0].backward(grad*self.obj_list[1].value*math.pow(self.obj_list[0].value, self.obj_list[1].value-1))#对底数求偏导
            self.obj_list[1].backward(grad*math.log(self.obj_list[0].value)*math.pow(self.obj_list[0].value, self.obj_list[1].value))#对指数求偏导
        elif self.op_type == 'log':#other 储存了底数
            self.obj_list[0].backward(grad/(self.obj_list[0].value*math.log(self.other[0])))
```

如代码中展示的，只需要实现这四类函数的求导就行了，虽然这样实现会导致效率非常非常低。

然后是定义一些运算操作:
```python
class AtomicOperation:#对于基本元素的运算，这部分在神经网络搭建的时候看不到，所以底层实现，怎么方便怎么来
    def sum(objs):
        m=0.0
        for obj in objs:
            m+=obj.value
        return MyFloat(m,require_grad=True,operation_graph=OperationGraph(op_type='sum',obj_list=objs))
    def multiply(objs):
        m=1.0
        for obj in objs:
            m=m*obj.value
        return MyFloat(m,require_grad=True,operation_graph=OperationGraph(op_type='multiply',obj_list=objs,other=[m]))
    def pow(obj1,obj2):
        if obj2.__class__.__name__ != 'MyFloat':
            index=MyFloat(obj2)
        else:
            index=obj2
        objy=math.pow(obj1.value, index)
        return MyFloat(m,require_grad=True,operation_graph=OperationGraph(op_type='power',obj_list=[obj1,index]))
    def sub(obj1,obj2):
        return AtomicOperation.sum([obj1,AtomicOperation.opposite(obj2)])
    def div(obj1,obj2):
        return AtomicOperation.multiply([obj1,AtomicOperation.pow(obj2, -1)])
    def log(obj,base=math.e):
        return MyFloat(math.log(obj.value,base=base),require_grad=True,operation_graph=OperationGraph(op_type='log',obj_list=[obj],other=[base]))
    def exp(obj):
        return AtomicOperation.pow(MyFloat(math.e),obj)
    def opposite(obj):
        return AtomicOperation.multiply([MyFloat(-1),obj])
```

# 2 实现Tensor及其运算
接下来需要定义的是更加高级的数据类型————张量Tensor。

```python
class Tensor:
    def __init__(self):

        self.shape = []

        self.data = []

        self.require_grad = False  # 假定这个属性不会被外部直接修改，否则得重写__setattr__函数

    def from_list(array) -> Tensor:  # 假设输入的array是list类型，并且假设非空

        def list2myflotlist(array):

            if array[0].__class__.__name__ == 'list':

                data = [list2myflotlist(a) for a in array]
            else:

                data = [MyFloat(float(a)) for a in array]

        def get_shape(array):

            if array[0].__class__.__name__ == 'list':

                shape = [len(array)]

                shape += get_shape(array[0])
            else:

                return [len(array)]

        t = Tensor()

        t.shape = get_shape(array)

        t.data = list2myflotlist(array)

        return t

    def require_grad_(self, value=True):

        def set_is_grad(array, value):

            if array[0].__class__.__name__ == 'list':
                for a in array:

                    set_is_grad(a)
            else:
                for a in array:

                    a.require_grad = value

        if value != self.require_grad:

            set_is_grad(self.data, value)

            self.require_grad = value

    def zero_grad(self):

        def set_grad(array):

            if array[0].__class__.__name__ == 'list':
                for a in array:

                    set_grad(a)
            else:
                for a in array:

                    a.grad = 0.0

        set_grad(self.data)

    def backward(self):#因为张量的运算结果还是张量，所以提供这个功能

        if len(self.shape) == 1 and self.shape[0] == 1:

            self.data[0].backward()
```

还需要定义张量的运算操作，包括加减乘除和矩阵乘和求和，还有对数和指数运算。

```python
class TensorOperation:

    def _mult_add(input) -> list:  # 假定非空，假定shape相同

        if input[0].__class__.__name__ == 'list':

            output = []

            for i in range(len(input[0])):

                output.append(TensorOperation._mult_add([a[i] for a in input]))
        else:

            output = AtomicOperation.sum(input)
        return output

    def _list_sum(input, dim=0) -> list:

        if dim > 0:

            output = []
            for a in input:

                output.append(TensorOperation._list_sum(a))
        else:

            output = TensorOperation._mult_add(input)
        return output

    def sum(input, dim=0) -> Tensor:  # 指的是内部的求和

        output = Tensor()

        output.require_grad = True

        # 基本运算的结果全都开了梯度

        if dim == 0 and len(input.shape) == 1:

            output.shape = [1]

            output.data = [TensorOperation._list_sum(input.data)]
        else:

            output.shape = input.shape

            del output.shape[dim]

            output.data = TensorOperation._list_sum(input.data)
        return output

    def _list_add(array1, array2) -> list:

        output = []

        if array2.__class__.__name__ == 'list':
            for i in range(len(array1)):
                output.append(TensorOperation._list_add(array1[i], array2[i]))
        else:
            if array1.__class__.__name__ == 'list':
                for a in array1:
                    output.append(TensorOperation._list_add(a, array2))
            else:
                output=AtomicOperation.sum([array1, array2])

        return output
    
    def add(tensor1, tensor2) -> Tensor:  # 假定shape相同

        output = Tensor()

        output.shape = tensor1.shape

        output.require_grad=True

        output.data = TensorOperation._list_add(tensor1.data, tensor2.data)
        return output

    def _list_opposite(array) -> list:

        output = []

        if input[0].__class__.__name__ == 'list':
            for a in array:

                output.append(TensorOperation._list_opposite(a))
        else:
            for a in array:

                output.append(AtomicOperation.opposite(a))
        return output

    def opposite(tensor) -> Tensor:

        output = Tensor()

        output.require_grad = True

        output.data = TensorOperation._list_opposite(tensor.data)

        output.shape = tensor.shape
        return output

    def sub(tensor1, tensor2) -> Tensor:  # 假定shape相同

        return TensorOperation.add(tensor1, TensorOperation.opposite(tensor2))

    def _list_div(array1, array2) -> list:

        output = []

        if array2.__class__.__name__ == 'MyFloat':

            if array1.__class__.__name__ == 'list':

                for a in array1:

                    output.append(TensorOperation._list_div(a, array2))
            else:
                output=AtomicOperation.div(array1, array2)
        else:

            if array1.__class__.__name__ == 'list':

                for i in range(len(array1)):

                    output.append(TensorOperation._list_div(

                        array1[i], array2[i]))

        return output

    def div(tensor1, tensor2) -> Tensor:

        output = Tensor()

        output.require_grad = True

        output.shape = tensor1.shape

        if len(tensor2.shape) == 1 and tensor2.shape[0] == 1:

            output.data = TensorOperation._list_div(
                tensor1.data, tensor2.data[0])
        else:

            output.data = TensorOperation._list_div(tensor1.data, tensor2.data)
        return output

    def _list_multiply(array1, array2) -> list:
        output=[]
        
        if array2.__class__.__name__ == 'list':
            for i in range(len(array1)):
                output.append(TensorOperation._list_multiply(array1[i],array2[i]))
        elif array1.__class__.__name__ == 'list':
            for i in range(len(array1)):
                output.append(TensorOperation._list_multiply(array1[i],array2))
        else:
            output=AtomicOperation.multiply([array1, array2])
        return output

    def multiply(tensor1, tensor2) -> Tensor:  # 元素对应乘法

        output = Tensor()

        output.require_grad = True

        output.shape = tensor1.shape

        if len(tensor2.shape) == 1 and tensor2.shape[0] == 1:

            output.data = TensorOperation._list_multiply(
                tensor1.data, tensor2.data[0])
        else:

            output.data = TensorOperation._list_multiply(
                tensor1.data, tensor2.data)
        return output

    def matrix_multiply(tensor1, tensor2) -> Tensor:  # 假设是可以相乘的两个二维数组
        """shape1=[m,n] shape2=[n,k]
        """
        output = Tensor()
        output.shape = [tensor1.shape[0], tensor2.shape[1]]
        output.require_grad = True
        output.data = [[0 for j in range(tensor2.shape[1])]
                       for i in range(tensor1.shape[0])]

        for i in range(tensor1.shape[0]):
            for j in range(tensor2.shape[1]):
                output.data[i][j] = AtomicOperation.sum([AtomicOperation.multiply(
                    [tensor1.data[i][n], tensor2.data[n][j]]) for n in range(tensor1.shape[1])])
        return output

    def _list_log(array, base=math.e) -> list:

        output = []

        if array[0].__class__.__name__ == 'list':
            for a in array:

                output.append(TensorOperation._list_log(a))
        else:
            for a in array:

                output.append(AtomicOperation.log(a, base=base))
        return output

    def log(tensor, base=math.e) -> Tensor:
        output = Tensor()

        output.require_grad = True

        output.data = TensorOperation._list_log(tensor.data, base)

        output.shape = tensor.shape
        return output

    def _list_exp(array) -> list:

        output = []

        if array[0].__class__.__name__ == 'list':
            for a in array:

                output.append(TensorOperation._list_exp(a))
        else:
            for a in array:

                output.append(AtomicOperation.exp(a))
        return output

    def exp(tensor) -> Tensor:
        output = Tensor()

        output.require_grad = True

        output.data = TensorOperation._list_exp(tensor.data)

        output.shape = tensor.shape
        return output

    def _list_pow(array, index=1) -> list:

        output = []

        if input[0].__class__.__name__ == 'list':
            for a in array:

                output.append(TensorOperation._list_pow(a))
        else:
            for a in array:

                output.append(AtomicOperation.pow(a, index))
        return output

    def pow(tensor, index=1) -> Tensor:

        output = Tensor()

        output.require_grad = True

        output.data = TensorOperation._list_pow(tensor.data, index)

        output.shape = tensor.shape
        return output

    def conv2d(imgs,kernels):
        """imgs.shape=[n,c,H,W] ,kernels.shape=[m,c,k,k]
        """
        n,c,H,W=imgs.shape
        m=kernels.shape[0]
        k=kernels.shape[2]
        output=Tensor()
        output.shape=[n,m,H-k+1,W-k+1]
        output.data=np.full([n,m,H-k+1,W-k+1], 0).tolist()
        for p in range(n):
            i=0
            while i+k <= H:
                j=0
                while j+k <= W:
                    subimg=[[[0 for ii in range(k)] for jj in range(k)] for cc in range(c)]
                    for q in range(c):
                        for u in range(k):
                            for v in range(k):
                                subimg[q][u][v]=imgs.data[p][q][i+u][j+v]
                    for mm in range(m):
                        dot=TensorOperation._list_multiply(subimg, kernels.data[mm])
                        dt=[]
                        for q in range(c):
                            for u in range(k):
                                for v in range(k):
                                    dt.append(dot[q][u][v])
                        output.data[p][mm][i][j]=AtomicOperation.sum(dt)
                    j+=1
                i+=1
        return output

    def maxpool2d(imgs,kernel_size):
        n,c,H,W=imgs.shape
        output=Tensor()
        output.shape=[n,c,int(H/kernel_size+0.5),int(W/kernel_size+0.5)]
        output.require_grad=True
        output.data=[[[[0 for kkk in range(int(W/kernel_size+0.5)) ] for kk in range(int(H/kernel_size+0.5))] for cc in range(c)] for nn in range(n)]
        
        for nn in range(n):
            for cc in range(c):
                lst=[[[] for i in range(int(W/kernel_size+0.5))] for j in range(int(H/kernel_size+0.5))]
                for i in range(H):
                    for j in range(W):
                        lst[int(i/kernel_size)][int(j/kernel_size)].append(imgs.data[nn][cc][i][j])
                for i in range(len(lst)):
                    for j in range(len(lst[i])):
                        maxindex=0
                        for k in range(1,len(lst[i][j])):
                            if lst[i][j][k].value > lst[i][j][maxindex].value:
                                maxindex=k
                        output.data[nn][cc][i][j]=lst[i][j][maxindex]
        return output

    def _list_relu(array):
        output=[]
        if array[0].__class__.__name__ == 'list':
            for a in array:
                output.append(TensorOperation._list_relu(a))
        else:
            z=MyFloat(0)
            for a in array:
                output.append(a if a.value > 0 else z)
        return output

    def relu(input):
        output=Tensor()
        output.shape=input.shape
        output.require_grad=True
        output.data=TensorOperation._list_relu(input.data)
        return output

    def softmax(input):#输入是二维
        output=Tensor()
        output.shape=input.shape
        output.require_grad=True
        output.data=[]
        for v in input.data:
            xp=TensorOperation._list_exp(v)
            sm=TensorOperation._list_sum(xp)
            dv=TensorOperation._list_div(xp,sm)
            output.data.append(dv)
        return output
    
    def _m2one(array):
        output=[]
        if array[0].__class__.__name__ == 'list':
            for a in array:
                output+=TensorOperation._m2one(a)
        else:
            for a in array:
                output.append(a)
        return output

    def _one2m(array,shape,index):
        output=[]
        idx=index
        if len(shape) > 1:
            for i in range(shape[0]):
                arr,idx=TensorOperation._one2m(array, shape[1:], idx)
                output.append(arr)
        else:
            for i in range(shape[0]):
                output.append(array[idx])
                idx += 1
        return output,idx

    def reshape(input,shape):#假设前后元素数量一致
        one=TensorOperation._m2one(input.data)#先变成一维的
        output=Tensor()
        output.shape=shape
        output.require_grad=True
        output.data,idx=TensorOperation._one2m(one, shape, 0)#从一维变成多维
        return output

    def add_bias(tensor1,tensor2):#为linear定制的函数，其实可以在linear里实现
        output=Tensor()
        output.shape=tensor1.shape
        output.require_grad=True
        output.data=[TensorOperation._list_add(a, tensor2.data) for a in tensor1.data]
        return output
```

# 3 定义网络基础结构
有了张量、张量运算，接下来就可以开始实现神经网络了。

首先要解决的是一些基础的网络结构，比如Linear、CNN、pool层、激活函数、softmax。

```python
class Linear:
    def __init__(self,in_dim,out_dim,bias=True):
        #使用随机数来初始化
        W=[[random.random()-0.5 for j in range(out_dim)] for i in range(in_dim)]
        self.W=Tensor.from_list(W)
        self.W.require_grad_(True)
        if bias:
            self.bias=Tensor.from_list([random.random()-0.5 for j in range(out_dim)])
            self.bias.require_grad_(True)
    def __call__(self,input):
        return self.forward(input)
    def forward(self,input):
        output=TensorOperation.matrix_multiply(input, self.W)
        if hasattr(self, 'bias'):
            output=TensorOperation.add_bias(output, self.bias)
        return output
    def parameters(self):
        if hasattr(self, 'bias'):
            return [self.W ,self.bias]
        else:
            return [self.W]

class Conv2d:
    def __init__(self,in_channels, out_channels, kernel_size):
        #使用随机数来初始化
        kernels=[[[[random.random()-0.5 for w in range(kernel_size)] for h in range(kernel_size)] for c in range(in_channels)] for o in range(out_channels)]
        self.kernels=Tensor.from_list(kernels)
        self.kernels.require_grad_(True)
    def __call__(self,input):
        return self.forward(input)
    def forward(self,input):
        return TensorOperation.conv2d(input,self.kernels)
    def parameters(self):
        return [self.kernels]

class MaxPool2d:
    def __init__(self,kernel_size):#简单点，不搞步长
        self.kernel_size=kernel_size
    def __call__(self,input):
        return self.forward(input)
    def forward(self,input):
        return TensorOperation.maxpool2d(input,self.kernel_size)
    def parameters(self):
        return [None]

class RelU:
    def __call__(self,input):
        return self.forward(input)
    def forward(self,input):
        return TensorOperation.relu(input)
    def parameters(self):
        return [None]

class Softmax:
    def __call__(self,input):
        return self.forward(input)
    def forward(self,input):
        return TensorOperation.softmax(input)
    def parameters(self):
        return [None]
```

# 4 定义网络结构

有了这些基础网络结构就能实现我们的模型。
```python
class net:
    def __init__(self):
        self.cnn=[
            Conv2d(1, 2, 3),
            RelU(),
            MaxPool2d(2),
            Conv2d(2, 4, 3),
            RelU(),
            MaxPool2d(2), 
            Conv2d(4, 4, 3),
            RelU(),
            MaxPool2d(2)
        ]
        self.fc=[
            Linear(16, 16),
            RelU(),
            Linear(16, 10),
            Softmax()
        ]
    def __call__(self,input):
        return self.forward(input)
    def forward(self,input):
        output=input
        for layer in self.cnn:
            output=layer(output)
        output=TensorOperation.reshape(output, [output.shape[0],16])
        for layer in self.fc:
            output=layer(output)
        return output
    def parameters(self):
        P=[]
        for layer in self.cnn:
            P+=layer.parameters()
        for layer in self.fc:
            P+=layer.parameters()
        return P
```

之所以用这么小的网络是因为：我们实现的这些运算效率非常低，网络稍微大一点就会导致训练时间过长。

我使用batch_size为2来训练，训练完大约一百万张mnist图像需要500多小时。

# 5 定义优化器

有了模型之后，我们还需要有优化器，这里简单实现一个SGD就行了。

```python
class optimizer:
    def __init__(self,parameters,lr=1e-3):
        self.parameters=parameters
        self.updata_num=1
    def step(self):
        def set_value(array,lr):
            if array[0].__class__.__name__ == 'list':
                for a in array:
                    set_value(a, lr)
            elif array[0].__class__.__name__ == 'MyFloat':
                for a in array:
                    a.value+=-lr*a.grad

        for par in self.parameters:
            if par is not None:
                set_value(par.data, self.lr/self.updata_num)
        self.updata_num+=1

    def zero_grad(self):
        for par in self.parameters:
            if par is not None:
                par.zero_grad()
```

# 6 定义损失函数和训练

再实现一下损失函数就行了。


```python
def CrossEntropyLoss(y,label):
    cost=[]
    for i in range(y.shape[0]):#因为是softmax,我们直接取相应的标签就行了，其他会自动变小
        cost.append(AtomicOperation.opposite(AtomicOperation.log(y.data[i][label[i]])))
    return AtomicOperation.sum(cost)
```
任何的步骤，基本和pytorch包的步骤差不多了。
```python
#****
optim.zero_grad()
pred=model(data)
loss=CrossEntropyLoss(pred,label)
loss.backward()
optim.step()
#****
```